مراحلی که در پروژه باید طی شود:
درک دیتا
-	فراخوانی دیتا ( با  panda) 
-	پلات کردن کلاس های دیتا ( با seaborn )
آماده سازی دیتا ( preprocess)
مدلسازی

پروژه دو بخش اصلی دارد
ساخت ماژول data_cleaner برای تمیزکاری دیتای train و dataset 
-	حذف کلمات انگلیسی و کارکتر های خاص
-	تبدیل اعداد به یک کارکتر مشخص (انگلیسی) (دلیلش رو نمیدونم)
-	تجزیه اخبار به کلمات یا کارکتر ها
-	محاسبه کلمات پر تکرار
-	جایگزینی کلمات کم تکرار با اسمی خاص (مثلا unk)
-	توکنایز کردن متن
-	بردار کردن متن (ساخت ماتریس ) یا روش های bag of words
•	هضم بدون nltk درست کار نمیکنه و nltk هم دیتا های زیادی داره که اگر از طریق کامند قصد دانلود داشته باشید خیلی طول میکشه و بهترین راه نصب دستی از طریق لینک (nltk data)  هستش (حجم فایل حدودا 650 مگ است)
	ساخت ماژول دسته بند
-	تابع clean که از ماژول ساخته شده قبلی می آید و دیتا رو پاکسازی میکند
...
